#!/usr/bin/env python3
# -*- coding: utf-8 -*-

###############################################################################
###############################################################################
###############################################################################
########    V1.0 2023/12/18     francescopiscitelli      ######################
###############################################################################
# automatically generated by the FlatBuffers compiler, do not modify

###############################################################################
###############################################################################

# import flatbuffers
# from flatbuffers.compat import import_numpy

import time
# import configargparse as argparse

# import matplotlib.pyplot as plt
import numpy as np
from confluent_kafka import Consumer, TopicPartition

# import os
import sys

from lib import libReadPcapngVMM as pcapr
from lib import libKafkaRX as krx 
from lib import libKafkaRawReadoutMessage as rawmsg

# import libReadPcapngVMM as pcapr
# import libKafkaRX as krx 
# import libKafkaRawReadoutMessage as rawmsg

###############################################################################
###############################################################################

class  kafka_reader():
    def __init__(self, NSperClockTick, nOfPackets = 1, broker = '127.0.0.1:9092', topic = 'freia_debug', MONTTLtype = True , MONring = 11, timeResolutionType = 'fine', sortByTimeStampsONOFF = False, operationMode = 'normal', testing = False):
        
        kaf = kafka_reader_preAlloc(NSperClockTick, nOfPackets, broker, topic, MONTTLtype, MONring, timeResolutionType,operationMode,testing)
        kaf.allocateMemory()
        kaf.read()
        self.readouts = kaf.readouts  
             
        if sortByTimeStampsONOFF is True:
       
                  print('Readouts are sorted by TimeStamp')
                 
                  self.readouts.sortByTimeStamps()
            
        else:
                
                  print('Readouts are NOT sorted by TimeStamp')
                
        self.readouts.calculateDuration()   


###############################################################################


class kafka_reader_preAlloc():
    def __init__(self, NSperClockTick, nOfPackets = 1, broker = '127.0.0.1:9092', topic = 'freia_debug', MONTTLtype = True , MONring = 11, timeResolutionType = 'fine',operationMode='normal',testing=False):
                
        self.NSperClockTick = NSperClockTick 
        self.nOfPackets     = nOfPackets
        self.broker         = broker 
        self.topic          = topic 
        self.MONTTLtype     = MONTTLtype
        self.MONring        = MONring
        self.timeResolutionType  = timeResolutionType
        self.operationMode       = operationMode

        #############################
        
        self.debug   = False
        
        self.removeremoveOtherDataTypesONOFF = True
        
        self.testing = testing
  
        self.rea = pcapr.pcapng_reader_PreAlloc(self.NSperClockTick, self.MONTTLtype, self.MONring, self.timeResolutionType, self.operationMode, kafkaStream = True)
            
        self.rea.timeResolutionType = self.timeResolutionType
        self.rea.operationMode      = self.operationMode
        
        self.readouts = self.rea.readouts
        
        #############################
        
        # self.fileSize   = self.nOfPackets*(self.rea.singleReadoutSize*self.rea.readoutsPerPacket+self.rea.ESSheaderSize)
        
        self.fileSize   = self.nOfPackets*(self.rea.singleReadoutSize*self.rea.readoutsPerPacket+32) # allocate more memory with header 32 instead of 30 
        
        if testing is True:
            print('\033[1;33mWARNING ---> Not streaming but testing locally enabled! \033[1;37m')
            print('simulating streaming {} packets ({:.1f} kbytes) from kafka'.format(self.nOfPackets,self.fileSize/1000))
        else:
            print('streaming {} packets ({:.1f} kbytes) from kafka'.format(self.nOfPackets,self.fileSize/1000))
        
        #############################        
        
    # def checkFWversion(self):
        
        
    
    
    def dprint(self, msg):
            if self.debug:
                print("{}".format(msg))
                
    def allocateMemory(self): 
        
        print('allocating memory',end='')
        
        numOfReadoutsTotal = self.nOfPackets*self.rea.readoutsPerPacket
        self.rea.counterCandidatePackets = self.nOfPackets
        self.rea.counterPackets          = self.nOfPackets
        self.rea.preallocLength          = round(numOfReadoutsTotal)
        self.dprint('\t preallocLength {}'.format(self.rea.preallocLength))
      
    def read(self):   
            
        print('\n',end='')
        
        if self.testing is False:

            kafka_config = krx.generate_config(self.broker, True)

            consumer = Consumer(kafka_config)
    
            metadata = krx.get_metadata_blocking(consumer)
            
            print('---> connection to kafka successful!')

            if self.topic not in metadata.topics:
                
                print('\033[1;31mERROR: --> Topic {} does not exist in topics list --> exiting...\n\033[1;37m'.format(self.topic),end='')
                sys.exit()
                # raise Exception("Topic {} does not exist in topics list".format(self.topic))
            
            if self.debug == True:
                print(metadata.topics) 
    
            
            topic_partitions = [TopicPartition(self.topic, p) for p in metadata.topics[self.topic].partitions]
            consumer.assign(topic_partitions)

 
            
        self.rea.overallDataIndex = 0 
        self.rea.data             = np.zeros((self.rea.preallocLength,19), dtype='int64') 
        
        self.rea.readouts.heartbeats = np.zeros((self.rea.counterCandidatePackets,), dtype='int64') 

        self.rea.stepsForProgress = int(self.rea.counterCandidatePackets/20)+1  # 4 means 25%, 50%, 75% and 100%
        
        indexPackets = 0 
        
        tStart = time.time() 
        flagTopicFound = True
        
        packetsFWversion = np.zeros((0),dtype='int64')
        
        for npack in range(self.nOfPackets):
                     
                    try:
                        
                        if flagTopicFound == True:
          
                            if self.testing is False:
                                while (msg := consumer.poll(timeout=0.5)) is None:
                                    time.sleep(0.2)
                                    timeElapsed = time.time() - tStart
                                    
                                    if timeElapsed > 3:
                                        print('\nwaiting for next comsumer poll on topic {}\033[1;37m'.format(self.topic))
                                        # flagTopicFound = False
                                        # break       
                                
                                ar52 = rawmsg.RawReadoutMessage.GetRootAs(msg.value(), 0)
                                packetData   = ar52.RawDataAsNumpy().tobytes()
                                packetLength = len(packetData) 
                                
                            else:
                              
                            #  if you want generated data to test
                                bytesGens = bytesGen()
                                packetLength = bytesGens.packetLength
                                packetData   = bytesGens.packetData
                                
                                # print(packetData)
                            
                       
                        
                    except:
                         self.dprint('--> other packet found')
                         print('\033[1;31mERROR: --> streaming failed ... exiting ... \n\033[1;37m',end='')
                         sys.exit()
                    else:
                        
                        indexESS = packetData.find(b'ESS')
                        
                        if indexESS != -1:
                           FWversionTemp    = self.rea.extractFWversion(packetData, indexESS)
                           packetsFWversion = np.append(packetsFWversion,FWversionTemp)
                                         
                           self.rea.checkFWversionSetHeaders(FWversionTemp)   
                        
                        if self.debug == True:
                            print('\npacket no. {} of {} received'.format(npack+1,self.nOfPackets),end='')
                        if flagTopicFound == True:    
                            self.rea.extractFromBytes(packetData,packetLength,indexPackets,debugMode = self.debug)
                            indexPackets += 1
        
                        
 
        if flagTopicFound == True: 
            
            print('[100%]',end=' ') 
            
            self.rea.checkIfUniformFWversion(packetsFWversion)
    
            self.dprint('\n All Packets {}, Candidates for Data {} --> Valid ESS {} (empty {}), NonESS  {} '.format(self.rea.counterPackets , self.rea.counterCandidatePackets,self.rea.counterValidESSpackets ,self.rea.counterEmptyESSpackets,self.rea.counterNonESSpackets))
              
            #######################################################       
                 
            ####here I remove  the rows that have been preallocated but no filled in case there were some packets big but no ESS
            if self.rea.preallocLength > self.rea.totalReadoutCount:
    
                datanew = np.delete(self.rea.data,np.arange(self.rea.totalReadoutCount,self.rea.preallocLength),axis=0)
                print('\nremoving extra allocated length not used ...')
                
            elif self.rea.preallocLength < self.rea.totalReadoutCount:
                print('something wrong with the preallocation: allocated length {}, total readouts {}'.format(self.preallocLength,self.rea.totalReadoutCount))
                sys.exit()
           
            elif self.rea.preallocLength == self.rea.totalReadoutCount:
                
                datanew = self.rea.data
                
            cz = pcapr.checkIfDataHasZeros(datanew)
            datanew = cz.dataOUT
            
            self.rea.readouts.transformInReadouts(datanew)
            
            # self.rea.readouts.heartbeats = self.rea.heartbeats
            # self.rea.readouts.removeNonESSpacketsHeartbeats()
    
            ############
            
            self.rea.checkTimeSettings()
            
            self.rea.timeAdjustedWithResolution()
                  
            ############
            
            print('\nkafka stream loaded - {} readouts - Packets: all {} (candidates {}) --> valid ESS {} (of which empty {}), nonESS {})'.format(self.rea.totalReadoutCount, self.rea.counterPackets,self.rea.counterCandidatePackets,self.rea.counterValidESSpackets ,self.rea.counterEmptyESSpackets,self.rea.counterNonESSpackets))    
            
            self.rea.removeOtherDataTypes(removeONOFF=self.removeremoveOtherDataTypesONOFF)
            
            self.readouts = self.rea.readouts
            
            # print(self.readouts.Channel)
            
        if self.testing is False:
           consumer.close()
 
###############################################################################
        
class bytesGen():
    def __init__(self):
        
        # self.packetLength = 30+20*446
        # # self.packetData   = b"\x00\x00\x45\x53\x53\x72"+os.urandom(self.packetLength-6)
        
        # self.packetData   = b"\x00\x00\x45\x53\x53\x72"+bytearray([1] * self.packetLength-6)

        # dataPath='../data/'
        
        dataPath='./data/'
        
        # print(dataPath)
        
        # dataPath = '/Users/francescopiscitelli/Documents/PYTHON/MBUTYcap/data/'
        
        # print(dataPath)
        
        with open(dataPath+'outputBinary1pkt', 'rb') as f: 

            temp =  self.packetData =  f.read()
           
        self.packetData =  temp[42:]
           
        self.packetLength = len(self.packetData)
        
        
        #  to write file from pcapr 
        # cont =+1 
        
        # if cont == 1 :
        #     # print(packetData)
        #     with open('/Users/francescopiscitelli/Documents/PYTHON/MBUTYcap_develKafka/lib/outputBin', 'wb') as f: 
        #         f.write(packetData)
                
        # else:
        #     with open('/Users/francescopiscitelli/Documents/PYTHON/MBUTYcap_develKafka/lib/outputBin', 'ab') as f: 
        #         f.write(packetData)
           
                
        # f.close()    
        
###############################################################################
###############################################################################

if __name__ == "__main__":
    
    NSperClockTick = 11.356860963629653  #ns per tick ESS for 88.0525 MHz
    
    
    # kaf = kafka_reader_preAlloc(NSperClockTick, nOfPackets = 1, broker = '127.0.0.1:9092', topic = 'freia_debug', MONTTLtype = True , \
    # MONring = 11, timeResolutionType = 'fine', operationMode='normal',testing=True)
                
    # kaf.allocateMemory()
    # kaf.read()
             
    topic = 'freia_debug'
    
    # topic = 'CAEN_debug'
    
    kaf =  kafka_reader(NSperClockTick, nOfPackets = 5, broker = '127.0.0.1:9092', topic = topic, MONTTLtype = True , \
    MONring = 11, timeResolutionType = 'fine', sortByTimeStampsONOFF = False, operationMode = 'normal', testing = True)
            # d
    
    readouts = kaf.readouts
    
    readoutsArray = readouts.concatenateReadoutsInArrayForDebug()
    
    
